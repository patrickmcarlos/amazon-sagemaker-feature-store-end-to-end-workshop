{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Working with Offline Feature Store \n",
    "## Features Store Dataset Extraction\n",
    "**How to use Amazon SageMaker Feature Store to retrieve and share machine learning (ML) features in order to build feature sets that can be used for training a ML model.**\n",
    "\n",
    "**Note:** Please set kernel to `Python 3 (Data Science)` and select instance to `ml.m5.4xlarge`\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Data Query](#Data-Query)\n",
    "    1. [Look in S3 console](#Look-in-S3-console)\n",
    "    1. [Query in Athena console](#Query-in-Athena-console)\n",
    "    1. [Query using SageMaker SDK](#Query-using-SageMaker-SDK)\n",
    "1. [Extract a feature set](#Extract-a-feature-set)    \n",
    "1. [Row level time travel](#Row-Level-Time-Travel)\n",
    "1. [Optional queries and validations](#Optional-queries-and-validations)\n",
    "    1. [Browse the set of offline store files in the S3 console](#Browse-the-set-of-offline-store-files-in-the-S3-console.)\n",
    "    1. [See the Glue tables that are used for Athena queries](#See-the-Glue-tables-that-are-used-for-Athena-queries.)\n",
    "    1. [Examine contents of a sample offline store Parquet file](#Examine-contents-of-a-sample-offline-store-Parquet-file.)\n",
    "    1. [Count the rows in an offline store](#Count-the-rows-in-an-offline-store.)\n",
    "    1. [Get a random sample of offline store rows](#Get-a-random-sample-of-offline-store-rows.)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "In previous module (`Module-1 Introduction to SageMaker Feature Store`), we demonstrated how to create multiple features groups inside a Amazon SageMaker Feature Store and ingest data into it.\n",
    "\n",
    "In this notebook, we will illustrate how to retrieve the ingested features from the multiple feature groups and combine them to build feature sets that can be used to train a ML model. We will cover the following aspects:\n",
    "\n",
    "* Look at data via S3 console (Offline feature store)\n",
    "* Athena query for dataset extraction - shown via Athena console\n",
    "* Athena query for dataset extraction (programmatically using SageMaker SDK)\n",
    "* Extract training dataset and persist to S3\n",
    "* Some additional queries to visualize a Parquet file, count the rows and get a random sample of offline store\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Query\n",
    "\n",
    "Before starting, in order to get into Feature Store in SageMaker Studio in the left side menu you need to select the SageMaker Components and registries, then select Feature Store from the list of components and click on the Feature store button.\n",
    "\n",
    "![Feature Store](../images/FS_0.png \"Feature Store\")\n",
    "\n",
    "First, let's start by looking at the feature store data. \n",
    "\n",
    "Once the features groups are created (as demonstrated in `Module-1`), we should be able to see the three features groups - `orders`, `products` and `customers` from the SageMaker Studio UI.\n",
    "\n",
    "![Features Groups](../images/FS1.png \"Features Groups\")\n",
    "\n",
    "Double click on one feature group to open a new correspondent tab in Studio. Inside one particular feature group, we can check the location of the offline data by looking for s3 on the bottom right like in the following image. \n",
    "\n",
    "![Orders FG](../images/FS2.png \"Orders FG\")\n",
    "\n",
    "### Look in S3 console\n",
    "\n",
    "In a different tab please open S3 console in your AWS account. The location of the offline feature store data is in S3 inside the default S3 bucket and prefix will respect the following format:\n",
    "\n",
    "s3://DEFAULT_BUCKET/sagemaker-feature-store/ACCOUNT_ID/sagemaker/REGION/offline-store/\n",
    "\n",
    "// Ours is `sagemaker-us-west-2-119387606724`\n",
    "\n",
    "![S3 Location](../images/S3_1.png \"S3 Location\")\n",
    "\n",
    "We notice that in S3 we have a timestamp suffix added for each feature group name and this corresponds to what is displayed in Athena console as tables.\n",
    "\n",
    "You can get the S3 URL from your environment after launching the cell referenced by: [Browse the set of offline store files in the S3 console.](#Browse-the-set-of-offline-store-files-in-the-S3-console.)\n",
    "\n",
    "Inside each feature group, we have a `data` subdirectory followed by directories partitioned by year/month/day/hour of Parquet files.\n",
    "\n",
    "![S3 Files](../images/S3_2.png \"S3 Files\")\n",
    "\n",
    "You will can get a look into a sample Parquet file after launching the cell referenced by: [Examine contents of a sample offline store Parquet file](#Examine-contents-of-a-sample-offline-store-Parquet-file.)\n",
    "\n",
    "---\n",
    "### Query in Athena console\n",
    "\n",
    "If it is for the first time we are launching Athena in AWS console we need to click on `Get Started` button and then before we run the first query we need to set up a query results location in Amazon S3. For simplicity, we can choose the same default SageMaker bucket that is used by Feature Store.\n",
    "\n",
    "![Athena results location](../images/AthenaSetupMessage.png \"Athena results location\")\n",
    "\n",
    "After setting the query results location, on the left panel we need to select the `AwsDataCatalog` as Data source and the `sagemaker_featurestore` as Database.\n",
    "\n",
    "We can run now run a query for the offline feature store data in Athena. To select the entries from the orders feature group we use the following SQL query. You will need to replace the orders table name with the corresponded value from your environment.\n",
    "\n",
    "```sql\n",
    "select * from \"<orders-feature-group-table-name>\"\n",
    "limit 20\n",
    "```\n",
    "\n",
    "![Athena Orders](../images/AthenaOrders.png \"Athena Orders\")\n",
    "\n",
    "You can notice that Feature Store adds the `write_time`, `api_invocation_time` and `is_deleted` fields to the offline store.\n",
    "\n",
    "Similarly, we can query the products and the customers feature group data by replacing the table name with the correspondent value. \n",
    "\n",
    "```sql\n",
    "select * from \"<products-feature-group-table-name>\"\n",
    "limit 20\n",
    "select * from \"<customers-feature-group-table-name>\"\n",
    "limit 20\n",
    "```\n",
    "\n",
    "Since our three features groups are related, we can do a join query by grouping all the information by product ID and by customer ID like here below. Please make sure you are replacing the table names with the corresponding values from your environment.\n",
    "\n",
    "```sql\n",
    "select *\n",
    "FROM\n",
    "  \"<orders-feature-group-table-name>\"\n",
    ", \"<products-feature-group-table-name>\"  \n",
    ", \"<customers-feature-group-table-name>\"\n",
    "WHERE (\"<orders-feature-group-table-name>\".\"customer_id\" = \"<customers-feature-group-table-name>\".\"customer_id\")\n",
    "AND (\"<orders-feature-group-table-name>\".\"product_id\" = \"<products-feature-group-table-name>\".\"product_id\")\n",
    "limit 20\n",
    "```\n",
    "\n",
    "![Athena Join](../images/AthenaJoin.png \"Athena Join\")\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query using SageMaker SDK\n",
    "\n",
    "Extract the data from Feature Store using Athena query using SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utilities import Utils\n",
    "from utilities import feature_store_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "account_id = sagemaker_session.account_id()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "query_results= 'sagemaker-featurestore-workshop'\n",
    "prefix = 'sagemaker-feature-store'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize boto3 runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "\n",
    "feature_store_session = sagemaker.Session(boto_session=boto_session, \n",
    "                                          sagemaker_client=sagemaker_client, \n",
    "                                          sagemaker_featurestore_runtime_client=featurestore_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retreive the orders, products and customers feature group names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive FG names\n",
    "%store -r customers_feature_group_name\n",
    "%store -r products_feature_group_name\n",
    "%store -r orders_feature_group_name\n",
    "# Retreive FG row count\n",
    "%store -r customers_count\n",
    "%store -r products_count\n",
    "%store -r orders_count\n",
    "\n",
    "customers_fg = FeatureGroup(name=customers_feature_group_name, sagemaker_session=feature_store_session)  \n",
    "products_fg = FeatureGroup(name=products_feature_group_name, sagemaker_session=feature_store_session)\n",
    "orders_fg = FeatureGroup(name=orders_feature_group_name, sagemaker_session=feature_store_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Athena join query to combine the 3 features groups - `customers`, `products` & `orders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_query = customers_fg.athena_query()\n",
    "customers_table = customers_query.table_name\n",
    "\n",
    "products_query = products_fg.athena_query()\n",
    "products_table = products_query.table_name\n",
    "\n",
    "orders_query = orders_fg.athena_query()\n",
    "orders_table = orders_query.table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'query_string' (str)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SELECT * FROM \"fscw-customers-02-15-23-31-1644968214\", \"fscw-products-02-15-23-31-1644968233\", \"fscw-orders-02-15-23-31-1644968254\" WHERE (\"fscw-orders-02-15-23-31-1644968254\".\"customer_id\" = \"fscw-customers-02-15-23-31-1644968214\".\"customer_id\") AND (\"fscw-orders-02-15-23-31-1644968254\".\"product_id\" = \"fscw-products-02-15-23-31-1644968233\".\"product_id\")'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_string = f'SELECT * FROM \"{customers_table}\", \"{products_table}\", \"{orders_table}\" ' \\\n",
    "               f'WHERE (\"{orders_table}\".\"customer_id\" = \"{customers_table}\".\"customer_id\") ' \\\n",
    "               f'AND (\"{orders_table}\".\"product_id\" = \"{products_table}\".\"product_id\")'\n",
    "%store query_string\n",
    "query_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Athena query output location: \n",
      "s3://sagemaker-us-west-2-119387606724/sagemaker-featurestore-workshop/query_results/\n"
     ]
    }
   ],
   "source": [
    "output_location = f's3://{default_bucket}/{query_results}/query_results/'\n",
    "print(f'Athena query output location: \\n{output_location}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check to see if data is available in offline store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Features are available in Offline Store!]\n"
     ]
    }
   ],
   "source": [
    "# Before extracting the data we need to check if the feature store was populated\n",
    "offline_store_contents = None\n",
    "while offline_store_contents is None:    \n",
    "    customers_total_record_count = Utils.get_historical_record_count(customers_feature_group_name)\n",
    "    products_total_record_count = Utils.get_historical_record_count(products_feature_group_name)\n",
    "    orders_total_record_count = Utils.get_historical_record_count(orders_feature_group_name)\n",
    "    if customers_total_record_count >= customers_count and \\\n",
    "        products_total_record_count >= products_count and \\\n",
    "        orders_total_record_count >= orders_count:\n",
    "        logger.info('[Features are available in Offline Store!]')\n",
    "        offline_store_contents = orders_total_record_count\n",
    "    else:\n",
    "        logger.info('[Waiting for data in Offline Store ...]')\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Athena query and load the output as a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>is_married</th>\n",
       "      <th>event_time</th>\n",
       "      <th>age_18-29</th>\n",
       "      <th>age_30-39</th>\n",
       "      <th>age_40-49</th>\n",
       "      <th>age_50-59</th>\n",
       "      <th>age_60-69</th>\n",
       "      <th>age_70-plus</th>\n",
       "      <th>...</th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id.1</th>\n",
       "      <th>product_id.1</th>\n",
       "      <th>purchase_amount</th>\n",
       "      <th>is_reordered</th>\n",
       "      <th>event_time.2</th>\n",
       "      <th>n_days_since_last_purchase</th>\n",
       "      <th>write_time.2</th>\n",
       "      <th>api_invocation_time.2</th>\n",
       "      <th>is_deleted.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C7252</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-15T23:29:48.477Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>O48491</td>\n",
       "      <td>C7252</td>\n",
       "      <td>P12484</td>\n",
       "      <td>0.530990</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-15T23:29:56.191Z</td>\n",
       "      <td>0.377907</td>\n",
       "      <td>2022-02-15 23:47:16.815</td>\n",
       "      <td>2022-02-15 23:43:54.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C6033</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-15T23:29:46.666Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>O36214</td>\n",
       "      <td>C6033</td>\n",
       "      <td>P1920</td>\n",
       "      <td>0.790792</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-15T23:29:55.483Z</td>\n",
       "      <td>0.296512</td>\n",
       "      <td>2022-02-15 23:47:16.524</td>\n",
       "      <td>2022-02-15 23:43:59.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C6033</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-15T23:29:46.666Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>O71172</td>\n",
       "      <td>C6033</td>\n",
       "      <td>P12428</td>\n",
       "      <td>0.209208</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-15T23:29:57.576Z</td>\n",
       "      <td>0.903101</td>\n",
       "      <td>2022-02-15 23:47:18.198</td>\n",
       "      <td>2022-02-15 23:43:08.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C6033</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-15T23:29:46.666Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>O94846</td>\n",
       "      <td>C6033</td>\n",
       "      <td>P16261</td>\n",
       "      <td>0.873465</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-15T23:29:58.983Z</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>2022-02-15 23:47:16.365</td>\n",
       "      <td>2022-02-15 23:42:42.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C6033</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-15T23:29:46.666Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>O4406</td>\n",
       "      <td>C6033</td>\n",
       "      <td>P9928</td>\n",
       "      <td>0.558416</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-15T23:29:53.643Z</td>\n",
       "      <td>0.312016</td>\n",
       "      <td>2022-02-15 23:47:16.638</td>\n",
       "      <td>2022-02-15 23:43:46.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id  sex  is_married                event_time  age_18-29  \\\n",
       "0       C7252    0           1  2022-02-15T23:29:48.477Z          1   \n",
       "1       C6033    1           1  2022-02-15T23:29:46.666Z          0   \n",
       "2       C6033    1           1  2022-02-15T23:29:46.666Z          0   \n",
       "3       C6033    1           1  2022-02-15T23:29:46.666Z          0   \n",
       "4       C6033    1           1  2022-02-15T23:29:46.666Z          0   \n",
       "\n",
       "   age_30-39  age_40-49  age_50-59  age_60-69  age_70-plus  ...  order_id  \\\n",
       "0          0          0          0          0            0  ...    O48491   \n",
       "1          1          0          0          0            0  ...    O36214   \n",
       "2          1          0          0          0            0  ...    O71172   \n",
       "3          1          0          0          0            0  ...    O94846   \n",
       "4          1          0          0          0            0  ...     O4406   \n",
       "\n",
       "  customer_id.1 product_id.1  purchase_amount is_reordered  \\\n",
       "0         C7252       P12484         0.530990            1   \n",
       "1         C6033        P1920         0.790792            0   \n",
       "2         C6033       P12428         0.209208            1   \n",
       "3         C6033       P16261         0.873465            0   \n",
       "4         C6033        P9928         0.558416            0   \n",
       "\n",
       "               event_time.2  n_days_since_last_purchase  \\\n",
       "0  2022-02-15T23:29:56.191Z                    0.377907   \n",
       "1  2022-02-15T23:29:55.483Z                    0.296512   \n",
       "2  2022-02-15T23:29:57.576Z                    0.903101   \n",
       "3  2022-02-15T23:29:58.983Z                    0.023256   \n",
       "4  2022-02-15T23:29:53.643Z                    0.312016   \n",
       "\n",
       "              write_time.2    api_invocation_time.2  is_deleted.2  \n",
       "0  2022-02-15 23:47:16.815  2022-02-15 23:43:54.000         False  \n",
       "1  2022-02-15 23:47:16.524  2022-02-15 23:43:59.000         False  \n",
       "2  2022-02-15 23:47:18.198  2022-02-15 23:43:08.000         False  \n",
       "3  2022-02-15 23:47:16.365  2022-02-15 23:42:42.000         False  \n",
       "4  2022-02-15 23:47:16.638  2022-02-15 23:43:46.000         False  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_query.run(query_string=query_string, output_location=output_location)\n",
    "orders_query.wait()\n",
    "joined_df = orders_query.as_dataframe()\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['customer_id', 'sex', 'is_married', 'event_time', 'age_18-29',\n",
       "       'age_30-39', 'age_40-49', 'age_50-59', 'age_60-69', 'age_70-plus',\n",
       "       'n_days_active', 'write_time', 'api_invocation_time', 'is_deleted',\n",
       "       'product_id', 'event_time.1', 'category_baby_food_formula',\n",
       "       'category_baking_ingredients', 'category_candy_chocolate',\n",
       "       'category_chips_pretzels', 'category_cleaning_products',\n",
       "       'category_coffee', 'category_cookies_cakes', 'category_crackers',\n",
       "       'category_energy_granola_bars', 'category_frozen_meals',\n",
       "       'category_hair_care', 'category_ice_cream_ice',\n",
       "       'category_juice_nectars', 'category_packaged_cheese',\n",
       "       'category_refrigerated', 'category_soup_broth_bouillon',\n",
       "       'category_spices_seasonings', 'category_tea',\n",
       "       'category_vitamins_supplements', 'category_yogurt', 'write_time.1',\n",
       "       'api_invocation_time.1', 'is_deleted.1', 'order_id', 'customer_id.1',\n",
       "       'product_id.1', 'purchase_amount', 'is_reordered', 'event_time.2',\n",
       "       'n_days_since_last_purchase', 'write_time.2', 'api_invocation_time.2',\n",
       "       'is_deleted.2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract a feature set\n",
    "\n",
    "**Note:** This extracted feature set will be used for model training in `Module-3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop columns which are not needed for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = joined_df.drop(['order_id', \n",
    "                           'customer_id', \n",
    "                           'product_id', \n",
    "                           'event_time', \n",
    "                           'write_time', \n",
    "                           'api_invocation_time', \n",
    "                           'is_deleted', \n",
    "                           'product_id.1', \n",
    "                           'event_time.1', \n",
    "                           'write_time.1', \n",
    "                           'api_invocation_time.1', \n",
    "                           'is_deleted.1', \n",
    "                           'customer_id.1', \n",
    "                           'purchase_amount',\n",
    "                           'event_time.2', \n",
    "                           'n_days_since_last_purchase',\n",
    "                           'write_time.2', \n",
    "                           'api_invocation_time.2', \n",
    "                           'is_deleted.2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>is_married</th>\n",
       "      <th>age_18-29</th>\n",
       "      <th>age_30-39</th>\n",
       "      <th>age_40-49</th>\n",
       "      <th>age_50-59</th>\n",
       "      <th>age_60-69</th>\n",
       "      <th>age_70-plus</th>\n",
       "      <th>n_days_active</th>\n",
       "      <th>category_baby_food_formula</th>\n",
       "      <th>...</th>\n",
       "      <th>category_ice_cream_ice</th>\n",
       "      <th>category_juice_nectars</th>\n",
       "      <th>category_packaged_cheese</th>\n",
       "      <th>category_refrigerated</th>\n",
       "      <th>category_soup_broth_bouillon</th>\n",
       "      <th>category_spices_seasonings</th>\n",
       "      <th>category_tea</th>\n",
       "      <th>category_vitamins_supplements</th>\n",
       "      <th>category_yogurt</th>\n",
       "      <th>is_reordered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.493151</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.189041</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.189041</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.189041</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.189041</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex  is_married  age_18-29  age_30-39  age_40-49  age_50-59  age_60-69  \\\n",
       "0    0           1          1          0          0          0          0   \n",
       "1    1           1          0          1          0          0          0   \n",
       "2    1           1          0          1          0          0          0   \n",
       "3    1           1          0          1          0          0          0   \n",
       "4    1           1          0          1          0          0          0   \n",
       "\n",
       "   age_70-plus  n_days_active  category_baby_food_formula  ...  \\\n",
       "0            0       0.493151                           0  ...   \n",
       "1            0       0.189041                           0  ...   \n",
       "2            0       0.189041                           0  ...   \n",
       "3            0       0.189041                           0  ...   \n",
       "4            0       0.189041                           0  ...   \n",
       "\n",
       "   category_ice_cream_ice  category_juice_nectars  category_packaged_cheese  \\\n",
       "0                       0                       0                         0   \n",
       "1                       0                       0                         0   \n",
       "2                       0                       0                         0   \n",
       "3                       0                       0                         0   \n",
       "4                       0                       0                         0   \n",
       "\n",
       "   category_refrigerated  category_soup_broth_bouillon  \\\n",
       "0                      0                             0   \n",
       "1                      0                             0   \n",
       "2                      0                             0   \n",
       "3                      0                             0   \n",
       "4                      0                             0   \n",
       "\n",
       "   category_spices_seasonings  category_tea  category_vitamins_supplements  \\\n",
       "0                           0             0                              0   \n",
       "1                           0             0                              0   \n",
       "2                           0             0                              0   \n",
       "3                           0             0                              0   \n",
       "4                           0             0                              0   \n",
       "\n",
       "   category_yogurt  is_reordered  \n",
       "0                1             1  \n",
       "1                0             0  \n",
       "2                0             1  \n",
       "3                0             0  \n",
       "4                0             0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write transformed features to local `data` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.././data/train/transformed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4e01ca194ba8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.././data/train/transformed.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m         )\n\u001b[0;32m-> 3204\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             )\n\u001b[1;32m    190\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.././data/train/transformed.csv'"
     ]
    }
   ],
   "source": [
    "model_df.to_csv('.././data/train/transformed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy file from local to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.././data/train/transformed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b3475aba8ce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_bucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transformed.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.././data/train/transformed.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mobject_upload_file\u001b[0;34m(self, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    279\u001b[0m     return self.meta.client.upload_file(\n\u001b[1;32m    280\u001b[0m         \u001b[0mFilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    130\u001b[0m         return transfer.upload_file(\n\u001b[1;32m    131\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             extra_args=ExtraArgs, callback=Callback)\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/boto3/s3/transfer.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[1;32m    285\u001b[0m             filename, bucket, key, extra_args, subscribers)\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;31m# If a client error was raised, add the backwards compatibility layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;31m# that raises a S3UploadFailedError. These specific errors were only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# however if a KeyboardInterrupt is raised we want want to exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# out of this and propogate the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# final result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/s3transfer/tasks.py\u001b[0m in \u001b[0;36m_main\u001b[0;34m(self, transfer_future, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;31m# Call the submit method to start submitting tasks to execute the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# transfer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_submit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m# If there was an exception raised during the submission of task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/s3transfer/upload.py\u001b[0m in \u001b[0;36m_submit\u001b[0;34m(self, client, config, osutil, request_executor, transfer_future, bandwidth_limiter)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;31m# Determine the size if it was not provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m             \u001b[0mupload_input_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovide_transfer_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;31m# Do a multipart upload if needed, otherwise do a regular put object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/s3transfer/upload.py\u001b[0m in \u001b[0;36mprovide_transfer_size\u001b[0;34m(self, transfer_future)\u001b[0m\n\u001b[1;32m    235\u001b[0m         transfer_future.meta.provide_transfer_size(\n\u001b[1;32m    236\u001b[0m             self._osutil.get_file_size(\n\u001b[0;32m--> 237\u001b[0;31m                 transfer_future.meta.call_args.fileobj))\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequires_multipart_upload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransfer_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/s3transfer/utils.py\u001b[0m in \u001b[0;36mget_file_size\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_file_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_chunk_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_byte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/genericpath.py\u001b[0m in \u001b[0;36mgetsize\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;34m\"\"\"Return the size of a file, reported by os.stat().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.././data/train/transformed.csv'"
     ]
    }
   ],
   "source": [
    "boto3.Session().resource('s3').Bucket(default_bucket).Object(os.path.join(query_results, 'transformed.csv')).upload_file('.././data/train/transformed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row Level Time Travel\n",
    "\n",
    "Features in different feature groups can be updated at different times. In order to build a training dataset based on specific event timestamps, we need a way to extract the feature values across all the feature group which are valid at the given timestamp using point-in-time queries. Data scientists can query for the exact set of feature values that would have been available at a specific time, without the chance of including data from beyond that time.\n",
    "\n",
    "[Refer to this blog post for details](https://aws.amazon.com/blogs/machine-learning/build-accurate-ml-training-datasets-using-point-in-time-queries-with-amazon-sagemaker-feature-store-and-apache-spark/)\n",
    "\n",
    "![Row Level Time Travel](../images/m2_nb1_row_level_time_travel.png \"Row Level Time Travel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we use a helper class containing many utility functions. The `get_features` method implements row-level time travel for us, taking in a feature set across multiple feature groups, and taking in an events dataframe with timestamps and identifiers (join keys). In this simple example, our offline store only has a single version of the data, so this is a trivial example. In a real-world example, the event timestamps would be distributed across history, and the time travel function would accurately pull the features from each feature group based on those timestamps. Note that we can either specify an explicit list of individual features, or we can use a wildcard to retrieve all features from a given feature group. `get_features` is a convenient way to gather a training dataset without having to write complicated SQL queries.\n",
    "\n",
    "Note - This is just demonstrating the use of row-level time travel, but the rest of the workshop does not depend upon the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a dataframe of transformed order data\n",
    "orders_df = pd.read_csv('.././data/transformed/orders.csv')\n",
    "\n",
    "multi_id_events = []\n",
    "\n",
    "#Iterating over the first 10 rows from the data frame\n",
    "for ind in orders_df.head(10).index:\n",
    "    event = [orders_df['event_time'][ind], orders_df['order_id'][ind],\n",
    "             orders_df['product_id'][ind], orders_df['customer_id'][ind]]\n",
    "    multi_id_events.append(event)\n",
    "    \n",
    "multi_id_df = pd.DataFrame(multi_id_events, columns=['my_event_time', 'order_id', 'product_id', 'customer_id' ])\n",
    "multi_id_df.head(10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.feature_store_helper import FeatureStore\n",
    "fs = FeatureStore()\n",
    "\n",
    "# Calling the helper function to return the feature values across multiple feature groups valid for the given \n",
    "# event timestamps. Note the usage of wildcard for the product feature group.\n",
    "\n",
    "\n",
    "fs.get_features(multi_id_df, 'my_event_time', \n",
    "                   features=[f'{customers_feature_group_name}:is_married',\n",
    "                             f'{products_feature_group_name}:*',\n",
    "                             f'{orders_feature_group_name}:purchase_amount'],\n",
    "               parallel=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional queries and validations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Browse the set of offline store files in the S3 console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Review customers offline store partitioned data files here: https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-west-2-119387606724?region=us-west-2&prefix=sagemaker-feature-store/119387606724/sagemaker/us-west-2/offline-store/fscw-customers-02-15-23-31-1644968214/data/\n",
      "\n",
      "Review products offline store partitioned data files here: https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-west-2-119387606724?region=us-west-2&prefix=sagemaker-feature-store/119387606724/sagemaker/us-west-2/offline-store/fscw-products-02-15-23-31-1644968233/data/\n",
      "\n",
      "Review orders offline store partitioned data files here: https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-west-2-119387606724?region=us-west-2&prefix=sagemaker-feature-store/119387606724/sagemaker/us-west-2/offline-store/fscw-orders-02-15-23-31-1644968254/data/\n"
     ]
    }
   ],
   "source": [
    "customers_s3_console_url = Utils.get_offline_store_url(customers_feature_group_name)\n",
    "products_s3_console_url = Utils.get_offline_store_url(products_feature_group_name)\n",
    "orders_s3_console_url = Utils.get_offline_store_url(orders_feature_group_name)\n",
    "\n",
    "logger.info('Review customers offline store partitioned data files here: '+customers_s3_console_url)\n",
    "logger.info('\\nReview products offline store partitioned data files here: '+products_s3_console_url)\n",
    "logger.info('\\nReview orders offline store partitioned data files here: '+orders_s3_console_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the Glue tables that are used for Athena queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To see the customers Glue table that was created for you, go here: https://console.aws.amazon.com/glue/home?region=us-west-2#table:catalog=119387606724;name=fscw-customers-02-15-23-31-1644968214;namespace=sagemaker_featurestore\n",
      "\n",
      "To see the products Glue table that was created for you, go here: https://console.aws.amazon.com/glue/home?region=us-west-2#table:catalog=119387606724;name=fscw-products-02-15-23-31-1644968233;namespace=sagemaker_featurestore\n",
      "\n",
      "To see the orders Glue table that was created for you, go here: https://console.aws.amazon.com/glue/home?region=us-west-2#table:catalog=119387606724;name=fscw-orders-02-15-23-31-1644968254;namespace=sagemaker_featurestore\n"
     ]
    }
   ],
   "source": [
    "customers_glue_console_url = Utils.get_glue_table_url(customers_feature_group_name)\n",
    "products_glue_console_url = Utils.get_glue_table_url(products_feature_group_name)\n",
    "orders_glue_console_url = Utils.get_glue_table_url(orders_feature_group_name)\n",
    "\n",
    "logger.info('To see the customers Glue table that was created for you, go here: '+customers_glue_console_url)\n",
    "logger.info('\\nTo see the products Glue table that was created for you, go here: '+products_glue_console_url)\n",
    "logger.info('\\nTo see the orders Glue table that was created for you, go here: '+orders_glue_console_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine contents of a sample offline store Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded sample Parquet file from offline store: 20220215T232952Z_05gT1OqbVV1GJoz7.parquet\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>is_married</th>\n",
       "      <th>event_time</th>\n",
       "      <th>age_18-29</th>\n",
       "      <th>age_30-39</th>\n",
       "      <th>age_40-49</th>\n",
       "      <th>age_50-59</th>\n",
       "      <th>age_60-69</th>\n",
       "      <th>age_70-plus</th>\n",
       "      <th>n_days_active</th>\n",
       "      <th>write_time</th>\n",
       "      <th>api_invocation_time</th>\n",
       "      <th>is_deleted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1262</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-15T23:29:39.490Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.570548</td>\n",
       "      <td>2022-02-15 23:46:22.993000+00:00</td>\n",
       "      <td>2022-02-15 23:41:22+00:00</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C4380</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-15T23:29:44.042Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.363699</td>\n",
       "      <td>2022-02-15 23:46:22.993000+00:00</td>\n",
       "      <td>2022-02-15 23:41:22+00:00</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C4383</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-15T23:29:44.047Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.969863</td>\n",
       "      <td>2022-02-15 23:46:22.993000+00:00</td>\n",
       "      <td>2022-02-15 23:41:22+00:00</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C2507</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-15T23:29:41.292Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.114384</td>\n",
       "      <td>2022-02-15 23:46:22.993000+00:00</td>\n",
       "      <td>2022-02-15 23:41:22+00:00</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-15T23:29:37.538Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.039726</td>\n",
       "      <td>2022-02-15 23:46:22.993000+00:00</td>\n",
       "      <td>2022-02-15 23:41:22+00:00</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id  sex  is_married                event_time  age_18-29  \\\n",
       "0       C1262    1           1  2022-02-15T23:29:39.490Z          0   \n",
       "1       C4380    0           1  2022-02-15T23:29:44.042Z          0   \n",
       "2       C4383    0           0  2022-02-15T23:29:44.047Z          0   \n",
       "3       C2507    1           1  2022-02-15T23:29:41.292Z          1   \n",
       "4         C43    0           0  2022-02-15T23:29:37.538Z          0   \n",
       "\n",
       "   age_30-39  age_40-49  age_50-59  age_60-69  age_70-plus  n_days_active  \\\n",
       "0          0          0          0          1            0       0.570548   \n",
       "1          0          0          0          1            0       0.363699   \n",
       "2          1          0          0          0            0       0.969863   \n",
       "3          0          0          0          0            0       0.114384   \n",
       "4          0          0          0          0            1       0.039726   \n",
       "\n",
       "                        write_time       api_invocation_time  is_deleted  \n",
       "0 2022-02-15 23:46:22.993000+00:00 2022-02-15 23:41:22+00:00       False  \n",
       "1 2022-02-15 23:46:22.993000+00:00 2022-02-15 23:41:22+00:00       False  \n",
       "2 2022-02-15 23:46:22.993000+00:00 2022-02-15 23:41:22+00:00       False  \n",
       "3 2022-02-15 23:46:22.993000+00:00 2022-02-15 23:41:22+00:00       False  \n",
       "4 2022-02-15 23:46:22.993000+00:00 2022-02-15 23:41:22+00:00       False  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look to the customers FG\n",
    "customers_sample_filename = Utils.download_sample_offline_file(customers_feature_group_name)\n",
    "logger.info('Downloaded sample Parquet file from offline store: '+customers_sample_filename+'\\n')\n",
    "\n",
    "customers_sample_df = pd.read_parquet(customers_sample_filename)\n",
    "customers_sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the rows in an offline store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found 10,000 records in \"fscw-customers-02-15-23-31\" feature group.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of rows from customers FG\n",
    "customers_total_record_count = Utils.get_historical_record_count(customers_feature_group_name)\n",
    "logger.info(f'Found {customers_total_record_count:,d} records in \"{customers_feature_group_name}\" feature group.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a random sample of offline store rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query :\n",
      " SELECT * FROM \"fscw-customers-02-15-23-31-1644968214\" tablesample bernoulli(25) limit 5\n",
      "On  database: sagemaker_featurestore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>is_married</th>\n",
       "      <th>event_time</th>\n",
       "      <th>age_18-29</th>\n",
       "      <th>age_30-39</th>\n",
       "      <th>age_40-49</th>\n",
       "      <th>age_50-59</th>\n",
       "      <th>age_60-69</th>\n",
       "      <th>age_70-plus</th>\n",
       "      <th>n_days_active</th>\n",
       "      <th>write_time</th>\n",
       "      <th>api_invocation_time</th>\n",
       "      <th>is_deleted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C2533</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-15T23:29:41.329Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.347260</td>\n",
       "      <td>2022-02-15 23:46:23.010</td>\n",
       "      <td>2022-02-15 23:41:22.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C9407</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-15T23:29:51.649Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028082</td>\n",
       "      <td>2022-02-15 23:46:23.010</td>\n",
       "      <td>2022-02-15 23:41:23.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C8797</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-15T23:29:50.759Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.247945</td>\n",
       "      <td>2022-02-15 23:46:23.010</td>\n",
       "      <td>2022-02-15 23:41:23.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-15T23:29:37.646Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.763699</td>\n",
       "      <td>2022-02-15 23:46:23.010</td>\n",
       "      <td>2022-02-15 23:41:23.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C733</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-15T23:29:38.719Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117123</td>\n",
       "      <td>2022-02-15 23:46:23.010</td>\n",
       "      <td>2022-02-15 23:41:23.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id  sex  is_married                event_time  age_18-29  \\\n",
       "0       C2533    1           0  2022-02-15T23:29:41.329Z          0   \n",
       "1       C9407    0           1  2022-02-15T23:29:51.649Z          0   \n",
       "2       C8797    1           1  2022-02-15T23:29:50.759Z          0   \n",
       "3         C98    0           0  2022-02-15T23:29:37.646Z          0   \n",
       "4        C733    0           0  2022-02-15T23:29:38.719Z          0   \n",
       "\n",
       "   age_30-39  age_40-49  age_50-59  age_60-69  age_70-plus  n_days_active  \\\n",
       "0          0          0          0          0            1       0.347260   \n",
       "1          0          0          0          1            0       0.028082   \n",
       "2          1          0          0          0            0       0.247945   \n",
       "3          1          0          0          0            0       0.763699   \n",
       "4          0          1          0          0            0       0.117123   \n",
       "\n",
       "                write_time      api_invocation_time  is_deleted  \n",
       "0  2022-02-15 23:46:23.010  2022-02-15 23:41:22.000       False  \n",
       "1  2022-02-15 23:46:23.010  2022-02-15 23:41:23.000       False  \n",
       "2  2022-02-15 23:46:23.010  2022-02-15 23:41:23.000       False  \n",
       "3  2022-02-15 23:46:23.010  2022-02-15 23:41:23.000       False  \n",
       "4  2022-02-15 23:46:23.010  2022-02-15 23:41:23.000       False  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get random rows from customers FG\n",
    "customers_sample_df = Utils.sample(customers_feature_group_name, n=5)\n",
    "customers_sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
